{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "3_Improving_a_Logistic_Regression_Model.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0731325603/Capstone/blob/master/3_Improving_a_Logistic_Regression_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC7DUZAyUpwT",
        "colab_type": "text"
      },
      "source": [
        "# Improving a Logistic Regression Model\n",
        "\n",
        "Â© Explore Data Science Academy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLqmYXG9UpwU",
        "colab_type": "text"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "- Understand feature subset selection as a general approach to model improvement;\n",
        "- Introduction to hyperparameter tuning;\n",
        "- Understand how to tune the logistic regression model hyperparameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-PUZVjIUpwV",
        "colab_type": "text"
      },
      "source": [
        "## Outline\n",
        "\n",
        "In the course of this tutorial, we will:\n",
        "\n",
        "1. Rebuild our Wisconsin Breast Cancer classifier;\n",
        "2. Use feature subset selection to create a sparse model;\n",
        "3. Tune the logistic regression model hyperparameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnNYWAs_UpwV",
        "colab_type": "text"
      },
      "source": [
        "## 1. Rebuilding the Breast Cancer Classifier\n",
        "\n",
        "Following on from the last tutorial, we'll load the breast cancer dataset (30 features - all continuous, one binary response - 0: malignant and 1: benign)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rypEIrKUUpwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Import dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# For precision, recall, etc.\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7wW4EdeUpwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = load_breast_cancer(return_X_y=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQyQWzO4Upwb",
        "colab_type": "text"
      },
      "source": [
        "It might be helpful to get into the habit of scaling your data during preprocessing for all machine learning tasks. Remember: _scaling_ puts all your features into the same range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXQjL1YjUpwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the scaler module\n",
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "\n",
        "# Scale data\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IMV9FCRUpwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q0-9mpWUpwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjbiUG_oUpwn",
        "colab_type": "text"
      },
      "source": [
        "## 2. Using Feature Subset Selection to Create a Sparse Model\n",
        "\n",
        "In general, given a particular model and a dataset, there are two categories of changes we can make to try and improve the model once it has been trained with default parameters:\n",
        "\n",
        "1. We can change/reduce which features the model was trained on; and/or\n",
        "2. We can tweak what are known as model hyperparameters.\n",
        "\n",
        "For both methods, the benchmark performance metrics we will use (within classification tasks) are precision, recall, and f1-score. In other words, after making our changes, we will compare the results for each of those metrics to the default model to determine if an improvement has been made.\n",
        "\n",
        "We'll start with the first method, and change which features the model is trained on. Assuming the default model was trained using all the features, any change we undertake will invariably reduce the number of features we use. A model trained using some subset of all the available features is known as a _sparse model_.\n",
        "\n",
        "We'll create four models to compare. In so doing, we'll cover some of the feature subset selection methods available for classification that you may be able to use in the course and your career going forward:\n",
        "\n",
        "1. Full model - using all features (reference model);\n",
        "2. Variance thresholding;\n",
        "3. Select K-best features;\n",
        "4. Sequential forward selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sDAN2InUpwp",
        "colab_type": "text"
      },
      "source": [
        "An important note here: always split your dataset into train/test folds _before_ performing feature selection. This prevents what is known as data leakage, where some of the 'unseen' data is exposed to the model during the training process, which would in turn inflate its test set accuracy later on. Take a quick read through the [tweet below](https://twitter.com/jmschreiber91/status/1137464236156702720?s=20) for some more context.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/split-tweet.jpg\" alt=\"train/test split ordering tweet\" width=\"400px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WLAoh4OUpwp",
        "colab_type": "text"
      },
      "source": [
        "For each method, we'll perform the feature selection on both the train and test sets - ensuring the model is tested on data with the same features it was trained on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieEIj2i0Upwq",
        "colab_type": "text"
      },
      "source": [
        "### 1. Full Model (No Selection)\n",
        "\n",
        "In order to have a reference model, we'll build a logistic regression classifier using all the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0_G_YkpUpwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm_full = LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chBwa5pqUpwv",
        "colab_type": "text"
      },
      "source": [
        "We'll need to standardise our features here to ensure the model is able to converge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDzNxyTRUpww",
        "colab_type": "code",
        "colab": {},
        "outputId": "6281ffbb-0b5a-43f2-cee6-96d750d9426a"
      },
      "source": [
        "# No transformations necessary\n",
        "lm_full.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO9konvLUpw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate predictions from full model\n",
        "pred_lm_full = lm_full.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6urvx5nOUpw3",
        "colab_type": "text"
      },
      "source": [
        "### 2. Selection by Variance Thresholding\n",
        "\n",
        "As you may recall, variance thresholding is a process by which we remove features which don't meet some minimum level of variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxFtpcJAUpw3",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-var-thresh.png\" alt=\"sketch-var-thresh\" width=\"500px;\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JmoiktFUpw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lm instance for var thresh\n",
        "lm_vt = LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTrCpYlTUpw6",
        "colab_type": "text"
      },
      "source": [
        "When we apply the transformation (i.e.: run feature selection) on the training set, we are fitting the transformation: `fit_transform(train_data)`. In order to apply the same transformation to the test set, we only need to transform it, not fit it: `transform(test_data)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o89qdJ_Upw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the var thresh model and choose a threshold\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "selector = VarianceThreshold(threshold=0.02)\n",
        "\n",
        "# Transform (i.e.: run selection on) the training data\n",
        "X_train_vt = selector.fit_transform(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxxIM6CBUpw8",
        "colab_type": "code",
        "colab": {},
        "outputId": "5c80a17e-38e3-4c81-dc4b-07a96e86ad1f"
      },
      "source": [
        "X_train_vt.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(455, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqw-7waaUpw-",
        "colab_type": "text"
      },
      "source": [
        "Using a variance threshold of 0.02, we have managed to trim the number of features in the training dataset from 30 down to 15."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgv2baprUpw_",
        "colab_type": "code",
        "colab": {},
        "outputId": "d67666de-76ec-412d-b02d-76b73aa9cec5"
      },
      "source": [
        "# Fit model to the transformed data\n",
        "lm_vt.fit(X_train_vt, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT6RHFplUpxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we'll need to remove the same features we did in the thresholding process\n",
        "X_test_vt = selector.transform(X_test)\n",
        "\n",
        "# Generate predictions from var thresh model\n",
        "pred_lm_vt = lm_vt.predict(X_test_vt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHbfKBm5UpxE",
        "colab_type": "text"
      },
      "source": [
        "### 3. Selection by Choosing K-Best Features\n",
        "\n",
        "`SelectKBest` is a feature selection module in `sklearn` which removes all but the _k_ highest scoring features according to some specified statistical test. In this case, we'll use `f_classif` as the test, which we specify in the `score_func` argument in the `SelectKBest` method.\n",
        "\n",
        "`f_classif` computes the ANOVA f-value between each label and feature - read up [more on it here](https://www.statisticshowto.com/probability-and-statistics/f-statistic-value-test/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHh-C66wUpxF",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-kbest.png\" alt=\"sketch-kbest\" width=\"600px;\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twEvQllkUpxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lm instance for k-best\n",
        "lm_kbest = LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBETiZItUpxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the feature selector module\n",
        "from sklearn import feature_selection\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "# Set up selector, choosing score function and number of features to retain\n",
        "selector_kbest = feature_selection.SelectKBest(score_func=f_classif, k=20)\n",
        "\n",
        "# Transform (i.e.: run selection on) the training data\n",
        "X_train_kbest = selector_kbest.fit_transform(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaDvHIIjUpxK",
        "colab_type": "code",
        "colab": {},
        "outputId": "18fc98e7-6370-4ab0-dbe3-d6febd02c212"
      },
      "source": [
        "X_train_kbest.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(455, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FglvAb-yUpxM",
        "colab_type": "code",
        "colab": {},
        "outputId": "bbb79d0b-ca1b-4926-e039-fbc19d697c54"
      },
      "source": [
        "# Fit model to the transformed data\n",
        "lm_kbest.fit(X_train_kbest, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF3yQp7uUpxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Before generating predictions, we'll need to transform the test data the same way we did the train data\n",
        "X_test_kbest = selector_kbest.transform(X_test)\n",
        "\n",
        "# Generate predictions from var thresh model\n",
        "pred_lm_kbest = lm_kbest.predict(X_test_kbest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM8jFjOVUpxQ",
        "colab_type": "text"
      },
      "source": [
        "### 4. Selection Using Sequential Forward Selection\n",
        "\n",
        "In this process, we iteratively fit the logistic regression classifier to our dataset, where our dataset starts with one feature, and adds one feature per iteration. For each iteration, k-fold cross-validation is used to compute the performance of the classifier. Thereafter, we can determine which number of features resulted in the optimal performance.\n",
        "\n",
        "We'll be using the `SequentialFeatureSelector` from a library known as `mlxtend`. Be sure to `pip install mlxtend` before you run the code below. We'll also be using f-score to compute the performance of each fitted model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbKbIYlkUpxR",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-sfs.png\" alt=\"sketch-sfs\" width=\"700px;\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-3fXIIuUpxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We begin by declaring the classifier we wish to use\n",
        "lm_sfs = LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gra-l8sPUpxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the selector module, and the accuracy_score module to computer performance\n",
        "from sklearn.metrics import f1_score\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as sfs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VikPVYQUpxV",
        "colab_type": "text"
      },
      "source": [
        "Take note of the arguments we pass to the SequentialFeatureSelector instance:\n",
        "\n",
        "- `lm_sfs` is the classifier we want it to use to fit to the data;\n",
        "- `k-features`is the maximum number of features we want to end up with. We set it higher rather than lower - in case the optimal number of features is higher than we may have thought;\n",
        "- `forward=True` sets the selector to use forward, as opposed to backward, selection;\n",
        "- `scoring='f1'` ensures performance is measured using an appropriate metric: here, f1-score;\n",
        "- `cv=10` means validation will be performed using k-fold cross validation, with k=10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf1sfZmNUpxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We then build our forward feature selector\n",
        "sfs = sfs(lm_sfs, k_features=10, forward=True, scoring='f1', cv=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co8uxwScUpxY",
        "colab_type": "text"
      },
      "source": [
        "The code cell below takes about a minute to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSnQNmdJUpxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sfs = sfs.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4CDIP5iUpxa",
        "colab_type": "code",
        "colab": {},
        "outputId": "acf80130-0bd7-49d7-e376-a3de056c29c1"
      },
      "source": [
        "# Plot the results\n",
        "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
        "\n",
        "fig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')\n",
        "\n",
        "plt.ylim([0.9, 1])\n",
        "plt.title('Sequential Forward Selection (w. StdDev)')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1i6fxLdUpxc",
        "colab_type": "text"
      },
      "source": [
        "Plotting the results we can see that performance begins to drop off after the 21st feature is added. Let's lock in those 21 features and use them to train our classifier. To be clear, these are not necessarily the first 21 features, but the 21 features which, when combined, produce optimal performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSPWsZ0tUpxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract the feature names for the optimal number using the dictionary provided by the subsets_ method\n",
        "columns = list(sfs.subsets_[30]['feature_idx'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAeg-KJcUpxe",
        "colab_type": "text"
      },
      "source": [
        "Having determined which number of features was best, we'll select those specific features and train the model using only them. Remember we need to select the features for both train _and_ test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQEhse-KUpxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_sfs = X_train[:, columns]\n",
        "X_test_sfs = X_test[:, columns]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eGd9IbNUpxg",
        "colab_type": "code",
        "colab": {},
        "outputId": "d49429c8-9cc4-46ef-be9e-255fe11d5ce2"
      },
      "source": [
        "X_train_sfs.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(455, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfJWHQoTUpxj",
        "colab_type": "text"
      },
      "source": [
        "Now we can train the model on this reduced training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQVmYONUUpxk",
        "colab_type": "code",
        "colab": {},
        "outputId": "d423b144-795f-435c-ceb2-6b6fd935fcae"
      },
      "source": [
        "lm_sfs.fit(X_train_sfs, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRjAiJMKUpxm",
        "colab_type": "text"
      },
      "source": [
        "Finally, we'll generate some predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZv293TPUpxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_lm_sfs = lm_sfs.predict(X_test_sfs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF3vo_hhUpxn",
        "colab_type": "text"
      },
      "source": [
        "### Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY7fTcNUUpxo",
        "colab_type": "text"
      },
      "source": [
        "Using `classification report`, we'll look at f1-score, precision and recall for each of our models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sWWH_ShUpxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Full Model')\n",
        "print(classification_report(y_test, pred_lm_full, target_names=['Malignant', 'Benign']))\n",
        "\n",
        "print('Var Thresh Model')\n",
        "print(classification_report(y_test, pred_lm_vt, target_names=['Malignant', 'Benign']))\n",
        "\n",
        "print('KBest Model')\n",
        "print(classification_report(y_test, pred_lm_kbest, target_names=['Malignant', 'Benign']))\n",
        "\n",
        "print('SFS Model')\n",
        "print(classification_report(y_test, pred_lm_sfs, target_names=['Malignant', 'Benign']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Eqd9xEkUpxq",
        "colab_type": "code",
        "colab": {},
        "outputId": "a36628cc-e55c-4ed7-97c3-deec7ffa9030"
      },
      "source": [
        "print('SFS Model')\n",
        "print(classification_report(y_test, pred_lm_sfs, target_names=['Malignant', 'Benign']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SFS Model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Malignant       1.00      0.90      0.95        39\n",
            "      Benign       0.95      1.00      0.97        75\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.97      0.95      0.96       114\n",
            "weighted avg       0.97      0.96      0.96       114\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUGRdJfJUpxs",
        "colab_type": "text"
      },
      "source": [
        "Interestingly, the full model achives perfect accuracy in both classes, a 0.07% improvement over the full model in the previous tutorial. The only difference here is that we scaled our data first - which clearly made a difference for the convergence of the logistic regression classifier.\n",
        "\n",
        "The other 3 models all perform very well, with the `SelectKBest` model also achieving perfect accuracy in both classes. \n",
        "\n",
        "The variance thresholding model had the poorest performance of the four. Perhaps using variance alone was not a sufficiently good indicator of the importance of the features in question.\n",
        "\n",
        "There is yet another method we can use to try and improve the model - let's take a look."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFOdKsnIUpxs",
        "colab_type": "text"
      },
      "source": [
        "## 3. Tuning the Model Hyperparameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTuG1vV4Upxs",
        "colab_type": "text"
      },
      "source": [
        "Most models have have settings or configuration values, known as hyperparameters, that can be changed to modify the fit of the model on the training data. The logistic regression model contains a hyperparameter _C_, which is used to control the penalty we apply to features that are less important (i.e.: more important features will have greater weight).\n",
        "\n",
        "The smaller the value of _C_, the greater the penalty to less important features. _C_ is a value greater than 0. You will recall from regression the regularisation methods Ridge and LASSO, which sought to minimise the L2- or L1-norm, respectively, by shrinking the coefficients of certain features. The penalty term had a coefficient of its own, which we called $\\alpha$, which controlled the severity of shrinkage that occurred. The _C_ value we will change here is precisely the _inverse_ value of $\\alpha$. In other words, the smaller the value of C, the greater the amount of shrinkage that occurs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a45Y9MS6Upxt",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8asY5fUUpxt",
        "colab_type": "text"
      },
      "source": [
        "We create three instances of the `LogisticRegression()` object, specifying a different value of the hyperparameter _C_ for each. When we set C to be small, we'll also specify the penalty type to be `l1` - this will allow some of the coefficients to be shrunk all the way to zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FJLBdyoUpxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Small C\n",
        "model_1 = LogisticRegression(C=0.05, penalty='l1', solver='liblinear', verbose=1)\n",
        "\n",
        "# Default C\n",
        "model_2 = LogisticRegression(C=1)\n",
        "\n",
        "# Large C\n",
        "model_3 = LogisticRegression(C=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mTMfd7MUpxv",
        "colab_type": "code",
        "colab": {},
        "outputId": "70740a4c-ffe0-4dc7-8319-d76d6485b23a"
      },
      "source": [
        "model_1.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.05, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=1,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UID_XEXgUpxy",
        "colab_type": "code",
        "colab": {},
        "outputId": "a0258066-2f69-4262-f54e-6c64b40bc1ee"
      },
      "source": [
        "model_2.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAfwTU0KUpxz",
        "colab_type": "code",
        "colab": {},
        "outputId": "50b58020-8679-4d1f-eb2d-56959cb8b6c9"
      },
      "source": [
        "model_3.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGZ5Rk4dUpx1",
        "colab_type": "text"
      },
      "source": [
        "Taking a look at the magnitudes of the coefficients of each trained model, we can see that the smaller C is, the more features are shrunk. In the case where `penalty='l1'`, smaller C values literally means less features included. Notice how only one non-zero coefficient remains in that case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeZ4gs36Upx1",
        "colab_type": "code",
        "colab": {},
        "outputId": "949632a7-9c98-4617-dc85-5302eca5a3b9"
      },
      "source": [
        "model_1.coef_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        , -2.07189122,  0.        ,  0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8CaHrmmUpx4",
        "colab_type": "code",
        "colab": {},
        "outputId": "23f91f5a-9a73-48dc-d707-4786734e9e9c"
      },
      "source": [
        "model_2.coef_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.7611167 , -1.65615376, -1.72064645, -1.49501402, -0.4804673 ,\n",
              "        -0.2554986 , -1.39564379, -1.88675522, -0.37475386,  0.92999426,\n",
              "        -1.14761616, -0.06882066, -0.86899982, -0.75098234, -0.04906747,\n",
              "         0.67561638,  0.07421836, -0.35106216,  0.44862657,  0.57753662,\n",
              "        -2.18677336, -2.23153836, -2.01294075, -1.58627796, -1.3795238 ,\n",
              "        -0.73360431, -1.45439135, -2.50448563, -1.11702906, -0.29772029]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP6IXmpbUpx6",
        "colab_type": "code",
        "colab": {},
        "outputId": "c461cfc7-98be-40a0-ba0c-9e63fecc8f9c"
      },
      "source": [
        "model_3.coef_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.59663513e+00, -3.29433429e+00, -2.46075904e+00,\n",
              "        -2.40279743e+00, -1.01940227e+00,  1.44142514e+00,\n",
              "        -2.88469115e+00, -3.58340794e+00,  4.64841674e-03,\n",
              "         1.78982561e+00, -4.55484934e+00, -2.63077826e-01,\n",
              "        -3.13595220e+00, -2.51883484e+00, -5.66192344e-01,\n",
              "         3.42656545e+00,  2.25994264e-01, -9.24069184e-01,\n",
              "         2.37840179e+00,  2.06612100e+00, -4.52483080e+00,\n",
              "        -4.85065639e+00, -3.87141643e+00, -3.40223131e+00,\n",
              "        -3.05293127e+00, -3.09352008e-01, -3.24281894e+00,\n",
              "        -3.92283784e+00, -2.67229807e+00, -8.17324890e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntkrxvD5Upx-",
        "colab_type": "text"
      },
      "source": [
        "### Predicting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnIsT09-Upx-",
        "colab_type": "text"
      },
      "source": [
        "The models have been trained. Let's make some predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_RE2_dAUpx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9LsMFuBUpyB",
        "colab_type": "text"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R6wRlvBUpyB",
        "colab_type": "text"
      },
      "source": [
        "We will once again use the classification report to compare the model results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NdlpMtLUpyC",
        "colab_type": "code",
        "colab": {},
        "outputId": "3ce67a5d-921d-4da1-915a-d744e8c7246c"
      },
      "source": [
        "print('C = 0.05')\n",
        "print(classification_report(y_test, pred_1, target_names=['Malignant', 'Benign']))\n",
        "\n",
        "print('C = 1')\n",
        "print(classification_report(y_test, pred_2, target_names=['Malignant', 'Benign']))\n",
        "\n",
        "print('C = 10')\n",
        "print(classification_report(y_test, pred_3, target_names=['Malignant', 'Benign']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C = 0.05\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Malignant       0.96      0.62      0.75        39\n",
            "      Benign       0.83      0.99      0.90        75\n",
            "\n",
            "    accuracy                           0.86       114\n",
            "   macro avg       0.90      0.80      0.83       114\n",
            "weighted avg       0.88      0.86      0.85       114\n",
            "\n",
            "C = 1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Malignant       1.00      1.00      1.00        39\n",
            "      Benign       1.00      1.00      1.00        75\n",
            "\n",
            "    accuracy                           1.00       114\n",
            "   macro avg       1.00      1.00      1.00       114\n",
            "weighted avg       1.00      1.00      1.00       114\n",
            "\n",
            "C = 10\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Malignant       0.97      0.97      0.97        39\n",
            "      Benign       0.99      0.99      0.99        75\n",
            "\n",
            "    accuracy                           0.98       114\n",
            "   macro avg       0.98      0.98      0.98       114\n",
            "weighted avg       0.98      0.98      0.98       114\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jco475zaUpyE",
        "colab_type": "text"
      },
      "source": [
        "Clearly, in setting C = 0.05 we have tuned the hyperparameter too aggressively and adversely affected the model's performance. Note how the f1-score is significantly lower for that variation than any other we have looked at.\n",
        "\n",
        "The logistic regression algorithm is a simple case for hyperparameter tuning because it only contains one value to tweak. Other models, like support vecto machines, tree-based methods, and neural networks, contain many more hyperparameters - in most cases too many for the model architect to optimise on her own. In these cases we use what is known as grid search: an iterative testing of multiple combinations of hyperparameters to find the optimal set. More on that later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBG43i5uUpyF",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In the course of this tutorial we have seen or been introduced to:\n",
        "\n",
        "- The importance of carrying out a train/test split before doing any feature selection;\n",
        "- Feature subset selection by variance thresholding;\n",
        "- Feature subset selection by select the k-best features;\n",
        "- Feature subset selection by sequential forward selection; and\n",
        "- Improving a model by tuning the hyperparameters."
      ]
    }
  ]
}